{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from import_utils import *\n",
    "\n",
    "gan_file_path = '../models/DCGAN.py'\n",
    "\n",
    "Discriminator, Generator, Encoder = import_names(gan_file_path, 'DCGAN', \n",
    "                                        ['Discriminator', 'Generator', 'Encoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "SEED = 0xDEADF00D\n",
    "IMAGE_SIZE = 64\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 10000\n",
    "LR = 0.0002\n",
    "LAMBDA = 0.3\n",
    "\n",
    "EMBEDDING_SIZE = 100\n",
    "\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.manual_seed(SEED)\n",
    "# torch.cuda.manual_seed_all(SEED)\n",
    "# np.random.seed(SEED)\n",
    "# random.seed(SEED)\n",
    "\n",
    "my_data_path = '../../data/faces'\n",
    "my_dataset = ImageFolder(my_data_path,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(IMAGE_SIZE),\n",
    "                          transforms.CenterCrop(IMAGE_SIZE),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                      ]))\n",
    "my_data_loader = DataLoader(my_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)\n",
    "\n",
    "den_data_path = '../../data/den_faces'\n",
    "den_dataset = ImageFolder(den_data_path,\n",
    "                      transform=transforms.Compose([\n",
    "                          transforms.Resize(IMAGE_SIZE),\n",
    "                          transforms.CenterCrop(IMAGE_SIZE),\n",
    "                          transforms.ToTensor(),\n",
    "                          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                      ]))\n",
    "den_data_loader = DataLoader(den_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [elem for elem in next(iter(my_data_loader))[0]]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(np.transpose(vutils.make_grid(batch, nrow=int(np.sqrt(BATCH_SIZE)), normalize=True), [1,2,0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [elem for elem in next(iter(den_data_loader))[0]]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.imshow(np.transpose(vutils.make_grid(batch, nrow=int(np.sqrt(BATCH_SIZE)), normalize=True), [1,2,0]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DTYPE = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n",
    "\n",
    "my_gen = Generator().cuda() if USE_CUDA else Generator()\n",
    "den_gen = Generator().cuda() if USE_CUDA else Generator()\n",
    "discr = Discriminator().cuda() if USE_CUDA else Discriminator()\n",
    "enc = Encoder().cuda() if USE_CUDA else Encoder()\n",
    "\n",
    "opt_my_g = Adam(my_gen.parameters(), lr=LR)\n",
    "opt_den_g = Adam(den_gen.parameters(), lr=LR)\n",
    "opt_d = Adam(discr.parameters(), lr=LR)\n",
    "opt_e = Adam(enc.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_norm(model):\n",
    "    total_norm = 0\n",
    "    for p in model.parameters():\n",
    "        param_norm = p.grad.data.norm(2)\n",
    "        total_norm += param_norm.item() ** 2\n",
    "    return total_norm ** (1. / 2)\n",
    "\n",
    "def plot_results():\n",
    "    my_batch = [elem for elem in next(iter(my_data_loader))[0]][:int(np.sqrt(BATCH_SIZE))]\n",
    "    den_batch = [elem for elem in next(iter(den_data_loader))[0]][:int(np.sqrt(BATCH_SIZE))]\n",
    "    my_batch = torch.stack(my_batch).type(DTYPE)\n",
    "    den_batch = torch.stack(den_batch).type(DTYPE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        my_enc, den_enc = enc(my_batch), enc(den_batch)\n",
    "        my_fake_batch = torch.cat([my_gen(my_enc), den_gen(my_enc)]).cpu()\n",
    "        den_fake_batch = torch.cat([den_gen(den_enc), my_gen(den_enc)]).cpu()\n",
    "        my_batch, den_batch = my_batch.cpu(), den_batch.cpu()\n",
    "\n",
    "    batch = torch.cat([my_batch, my_batch, my_fake_batch, \n",
    "                       den_batch, den_batch, den_fake_batch])\n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.imshow(np.transpose(vutils.make_grid(batch, nrow=int(np.sqrt(BATCH_SIZE)) * 2, \n",
    "                                             normalize=True), [1,2,0]))\n",
    "    plt.show()\n",
    "    \n",
    "criterion = nn.BCELoss()\n",
    "mae_loss = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(batch, _discr, _gen, _enc, _opt_d, _opt_g, _opt_e):\n",
    "    real_batch = batch.type(DTYPE)\n",
    "    \n",
    "    # Discriminator step\n",
    "    _discr.zero_grad()\n",
    "    out_real = _discr(real_batch).squeeze()\n",
    "    out_fake = _discr(_gen(_enc(real_batch))).squeeze()\n",
    "\n",
    "    real_labels = .995 + torch.rand(real_batch.shape[0]).type(DTYPE) * .005\n",
    "    fake_labels = torch.rand(real_batch.shape[0]).type(DTYPE) * .005\n",
    "\n",
    "    loss_d = criterion(out_real, real_labels)\n",
    "    loss_d += criterion(out_fake, fake_labels)\n",
    "    loss_d.backward()\n",
    "    _opt_d.step()\n",
    "\n",
    "    # Generator + encoder step\n",
    "    _gen.zero_grad()\n",
    "    _enc.zero_grad()\n",
    "\n",
    "    reconstruction = _gen(_enc(real_batch))\n",
    "    out_discr = _discr(reconstruction).squeeze()\n",
    "\n",
    "    loss_g = criterion(out_discr, real_labels)\n",
    "    reconstruction_loss = mae_loss(reconstruction, real_batch)\n",
    "    f_loss = LAMBDA * reconstruction_loss + loss_g\n",
    "    f_loss.backward()\n",
    "    _opt_g.step()\n",
    "    _opt_e.step()\n",
    "    \n",
    "    return loss_d, loss_g, reconstruction_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from IPython import display\n",
    "\n",
    "den_gen_loss, my_gen_loss, discr_loss, enc_loss = [], [], [], []\n",
    "start_time = time()\n",
    "\n",
    "for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "    tmp_den_gen_loss, tmp_my_gen_loss, tmp_discr_loss, tmp_enc_loss = [], [], [], []\n",
    "    \n",
    "    for (my_batch, _), (den_batch, _) in zip(my_data_loader, den_data_loader):\n",
    "        my_d_loss, my_g_loss, my_e_loss = train_step(my_batch, discr, my_gen, \n",
    "                                                     enc, opt_d, opt_my_g, opt_e)\n",
    "        den_d_loss, den_g_loss, den_e_loss = train_step(den_batch, discr, den_gen, \n",
    "                                                        enc, opt_d, opt_den_g, opt_e)\n",
    "        \n",
    "        tmp_enc_loss.append((my_e_loss.item() + den_e_loss.item()) / 2)\n",
    "        tmp_discr_loss.append((my_d_loss.item() + den_d_loss.item()) / 2)\n",
    "        tmp_my_gen_loss.append(my_g_loss.item())\n",
    "        tmp_den_gen_loss.append(den_g_loss.item())\n",
    "    \n",
    "    my_gen_loss.append(np.mean(tmp_my_gen_loss))\n",
    "    den_gen_loss.append(np.mean(tmp_den_gen_loss))\n",
    "    discr_loss.append(np.mean(tmp_discr_loss))\n",
    "    enc_loss.append(np.mean(tmp_enc_loss))\n",
    "    \n",
    "    display.clear_output(wait=True)\n",
    "    plot_results()\n",
    "    plt.plot(my_gen_loss, label='My generator loss')\n",
    "    plt.plot(den_gen_loss, label='Den generator loss')\n",
    "    plt.plot(discr_loss, label='Discriminator loss')\n",
    "    plt.show()\n",
    "\n",
    "print(f'Training took {time() - start_time} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
